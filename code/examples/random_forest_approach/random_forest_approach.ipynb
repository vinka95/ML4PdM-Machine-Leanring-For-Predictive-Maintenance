{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Approach\r\n",
    "\r\n",
    "This Jupyter notebook provides a full example of the Random Forest Approach.\r\n",
    "\r\n",
    "## Imports and setup:\r\n",
    "\r\n",
    "The following code cell deals with all the imports and initial setup. The seed of the numpy random number generator is fixed to create reproducible results and the ray-tune framework is initialized."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from typing import List\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import ray\r\n",
    "from ray import tune\r\n",
    "from ray.tune.suggest.bohb import TuneBOHB\r\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\r\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "\r\n",
    "from ml4pdm.data import Dataset\r\n",
    "from ml4pdm.evaluation import Evaluator\r\n",
    "from ml4pdm.evaluation.metrics import loss_asymmetric, loss_false_negative_rate, loss_false_positive_rate, score_performance\r\n",
    "from ml4pdm.parsing import DatasetParser\r\n",
    "from ml4pdm.prediction import EnsembleApproach, WindowedPredictor\r\n",
    "from ml4pdm.transformation import WindowingApproach, AttributeFilter, SklearnWrapper\r\n",
    "\r\n",
    "np.random.seed(2)\r\n",
    "ray.init(include_dashboard=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.31.31',\n",
       " 'raylet_ip_address': '192.168.31.31',\n",
       " 'redis_address': '192.168.31.31:6379',\n",
       " 'object_store_address': 'tcp://127.0.0.1:65527',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:65250',\n",
       " 'webui_url': None,\n",
       " 'session_dir': 'C:\\\\Users\\\\CHRIST~1\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2021-08-25_23-43-38_371426_23832',\n",
       " 'metrics_export_port': 65286,\n",
       " 'node_id': 'c67c697361d914c0634d398a8305b2bebc7084974bcaf7316cf45cfe'}"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare dataset:\r\n",
    "\r\n",
    "The base for the datasets is the CMAPSS FD001. The train and test datasets are prepared by removing non-changing as well as settings features. After that a min max scaling is also applied per feature. Windowing is applied to be able to pass single timesteps to the Decision Tree. The labels are computed per timestep by adding the distance to the end onto the given target for an entire instance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train_dataset, test_dataset = DatasetParser.get_cmapss_data(test=True)\r\n",
    "train_dataset = AttributeFilter.remove_features(train_dataset, [1, 2, 3, 4, 8, 13, 19, 22])\r\n",
    "test_dataset = AttributeFilter.remove_features(test_dataset, [1, 2, 3, 4, 8, 13, 19, 22])\r\n",
    "\r\n",
    "preprocessing = make_pipeline(SklearnWrapper(MinMaxScaler(), SklearnWrapper.extract_timeseries_concatenated, SklearnWrapper.rebuild_timeseries_concatenated), WindowingApproach(1), \"passthrough\")\r\n",
    "\r\n",
    "train_instance_lengths = WindowedPredictor.extract_instance_lengths(train_dataset)\r\n",
    "test_instance_lengths = WindowedPredictor.extract_instance_lengths(test_dataset)\r\n",
    "\r\n",
    "train_dataset = preprocessing.fit_transform(train_dataset)\r\n",
    "test_dataset = preprocessing.transform(test_dataset)\r\n",
    "\r\n",
    "def annotate_single_timesteps(data: Dataset, instance_lengths: List[int]):\r\n",
    "    new_targets = []\r\n",
    "    start = 0\r\n",
    "    for i, instance_len in enumerate(instance_lengths):\r\n",
    "        for j in range(start, start+instance_len):\r\n",
    "            if len(data.target) > i:\r\n",
    "                new_targets.append(data.target[i] + instance_len - j)\r\n",
    "            else:\r\n",
    "                new_targets.append(instance_len - j)\r\n",
    "    data.target = new_targets\r\n",
    "\r\n",
    "\r\n",
    "annotate_single_timesteps(train_dataset, train_instance_lengths)\r\n",
    "annotate_single_timesteps(test_dataset, test_instance_lengths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter optimization:\r\n",
    "\r\n",
    "The following code cell performs hyperparameter optimization using the ray-tune framework. The parameters \"n_elements\", \"num_samples\", \"max_features\", \"splitter\" and \"criterion\" are optimized using the asymmetric loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pipeline_training(config, data=None):\r\n",
    "    rfa = EnsembleApproach(config[\"n_elements\"], DecisionTreeRegressor, fit_preprocessing=EnsembleApproach.random_sampling(config[\"num_samples\"]),\r\n",
    "                           max_features=config[\"max_features\"], splitter=config[\"splitter\"], criterion=config[\"criterion\"])\r\n",
    "    evaluator = Evaluator(None, [rfa], None, [loss_asymmetric, mean_squared_error])\r\n",
    "    results = evaluator.evaluate_train_test_split(data[\"train_dataset\"], data[\"test_dataset\"])[0]\r\n",
    "    tune.report(loss=results[0], mse=results[1])\r\n",
    "\r\n",
    "\r\n",
    "data_dict = {\r\n",
    "    \"train_dataset\": train_dataset,\r\n",
    "    \"test_dataset\": test_dataset,\r\n",
    "}\r\n",
    "\r\n",
    "algo = TuneBOHB(seed=2)\r\n",
    "\r\n",
    "analysis = tune.run(\r\n",
    "    tune.with_parameters(pipeline_training, data=data_dict),\r\n",
    "    search_alg=algo,\r\n",
    "    metric=\"loss\",\r\n",
    "    mode=\"min\",\r\n",
    "    num_samples=-1,\r\n",
    "    time_budget_s=int(90*60),\r\n",
    "    resources_per_trial={\"cpu\": 2},\r\n",
    "    config={\r\n",
    "        \"n_elements\": tune.choice(range(1, 50)),\r\n",
    "        \"num_samples\": tune.choice(range(1000, int(0.75*len(train_dataset.data)), 500)),\r\n",
    "        \"max_features\": tune.choice(range(1, 16)),\r\n",
    "        \"splitter\": tune.choice([\"best\", \"random\"]),\r\n",
    "        \"criterion\": tune.choice([\"mse\", \"mae\"]),\r\n",
    "    }\r\n",
    ")\r\n",
    "\r\n",
    "best_config = analysis.get_best_config(metric=\"loss\", mode=\"min\")\r\n",
    "print(\"Best config: \", best_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate Approach:\r\n",
    "\r\n",
    "The best config that was obtained in the above hyperparameter optimization is evaluated using various metrics. This allows for comparing it with other approaches."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "rfa = EnsembleApproach(43, DecisionTreeClassifier, fit_preprocessing=EnsembleApproach.random_sampling(14000),\r\n",
    "                           max_features=15, splitter=\"best\", criterion=\"gini\")\r\n",
    "evaluator = Evaluator(None, [rfa], None, [loss_asymmetric, mean_squared_error, score_performance, mean_absolute_error,\r\n",
    "                                          mean_absolute_percentage_error, loss_false_positive_rate, loss_false_negative_rate])\r\n",
    "\r\n",
    "results = evaluator.evaluate_train_test_split(train_dataset, test_dataset)[0]\r\n",
    "for i in [2, 4, 5, 6]:\r\n",
    "    results[i] *= 100 \r\n",
    "print(\"S:\\t{:.2f}\\nMSE:\\t{:.2f}\\nA(%):\\t{:.2f}\\nMAE:\\t{:.2f}\\nMAPE:\\t{:.2f}\\nFPR(%):\\t{:.2f}\\nFNR(%):\\t{:.2f}\".format(*results))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S:\t190435757706.53\n",
      "MSE:\t7854.98\n",
      "A(%):\t6.72\n",
      "MAE:\t75.95\n",
      "MAPE:\t42.76\n",
      "FPR(%):\t10.16\n",
      "FNR(%):\t83.12\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('ml4pdm': conda)"
  },
  "interpreter": {
   "hash": "518b1f15e5ecc8bbb5b8b6df33eabffa3a6ba03d62816d8dcf0452a3afe449fb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}